{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import sys\n",
    "# from tqdm.asyncio import tqdm\n",
    "from tqdm import tqdm\n",
    "import gzip\n",
    "import csv\n",
    "import aiohttp\n",
    "import asyncio\n",
    "import pandas as pd\n",
    "\n",
    "total = 2918269#len(pd.read_csv(\"F:\\\\_Bounty\\\\humana_payers.csv\"))\n",
    "filename = \"F:\\\\_Bounty\\\\humana_payers.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_size(session, semaphore, queue):\n",
    "    while True:\n",
    "        url = await queue.get()\n",
    "        if url is None:\n",
    "            break\n",
    "\n",
    "\n",
    "        # if \"https\" not in url: # for Humana only\n",
    "        url = \"https://storage.googleapis.com/cms-humana-price-transparency-prd/prod/february/inn/\" + url.replace(\"?fileType=innetwork&filename=\", \"\")\n",
    "        print(url)\n",
    "\n",
    "        try:\n",
    "            async with semaphore, session.get(url, ssl=False) as r:\n",
    "                print(r.status)\n",
    "                size = r.headers.get(\"Content-Length\")\n",
    "        except:\n",
    "            size = None\n",
    "\n",
    "        await queue.put((url, size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main():\n",
    "    async with aiohttp.ClientSession(raise_for_status=True) as session:\n",
    "        semaphore = asyncio.Semaphore(50)  # Limit to 50 concurrent requests\n",
    "\n",
    "        # Create the queue and fill it with URLs\n",
    "        queue = asyncio.Queue()\n",
    "        for chunk in tqdm(pd.read_csv(filename, chunksize=10000), total=total, desc=\"Chunks\"):\n",
    "            for url in chunk[\"url\"]:\n",
    "                queue.put_nowait(url)\n",
    "\n",
    "        # Create a fixed number of worker coroutines to process the URLs\n",
    "        num_workers = 50\n",
    "        tasks = []\n",
    "        for i in range(num_workers):\n",
    "            tasks.append(asyncio.create_task(get_size(session, semaphore, queue)))\n",
    "\n",
    "        pbar = tqdm(total = total, desc=\"Tasks\")\n",
    "        # Wait for all workers to finish processing the URLs\n",
    "        for task in tasks:\n",
    "            await task\n",
    "            pbar.update(1)\n",
    "\n",
    "        # Extract the results from the queue and add them to the DataFrame\n",
    "        results = []\n",
    "        while not queue.empty():\n",
    "            url, size = await queue.get()\n",
    "            results.append((url, size))\n",
    "        df = pd.DataFrame(results, columns=[\"url\", \"size\"])\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunks:   0%|          | 292/2918269 [00:05<16:38:29, 48.71it/s]\n",
      "                                                  \r"
     ]
    }
   ],
   "source": [
    "df1 = await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.sort_values(by=\"size\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv(\"F:\\\\_Bounty\\\\humana_payers_sorted.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scraper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9f367541eacc62922b0d667875f754c11ddf0e046a966297271f755050d7d39a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
